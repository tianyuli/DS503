{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sc = new SparkContext(...)\n",
    "  // 假定邻居页面的List存储为Spark objectFile\n",
    "  val links = sc.objectFile[(String, Seq[String])](\"links\")\n",
    "    .partitionBy(new HashPartitioner(100))\n",
    "    .persist()\n",
    "\n",
    "  //设置页面的初始rank值为1.0\n",
    "  var ranks = links.mapValues(_ => 1.0)\n",
    "\n",
    "  //迭代10次\n",
    "  for (i <- 0 until 10) {\n",
    "    val contributions = links.join(ranks).flatMap {\n",
    "      case (pageId, (links, rank)) =>\n",
    "        //注意此时的links为模式匹配获得的值，类型为Seq[String]，并非前面读取出来的页面List\n",
    "        links.map(dest => (dest, rank / links.size))\n",
    "    }\n",
    "    //简化了的rank计算公式\n",
    "    ranks = contributions.reduceByKey(_ + _).mapValues(0.15 + 0.85 * _)\n",
    "  }\n",
    "  ranks.saveAsTextFile(\"ranks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.174.130:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1572968146256)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "linkage: org.apache.spark.rdd.RDD[String] = soc-LiveJournal1.txt MapPartitionsRDD[1] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val linkage = sc.textFile(\"soc-LiveJournal1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lineLengths: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at map at <console>:26\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lineLengths = linkage.map(s => s.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# FromNodeId\tToNodeId\n",
      "0\t1\n",
      "0\t2\n",
      "0\t3\n",
      "0\t4\n",
      "0\t5\n",
      "0\t6\n",
      "0\t7\n",
      "0\t8\n",
      "0\t9\n"
     ]
    }
   ],
   "source": [
    "linkage.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "lineLengths.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "25: error: value option is not a member of org.apache.spark.rdd.RDD[String]",
     "output_type": "error",
     "traceback": [
      "<console>:25: error: value option is not a member of org.apache.spark.rdd.RDD[String]",
      "       val test = sc.textFile(\"test.txt\").option(\"header\", \"true\")",
      "                                          ^",
      ""
     ]
    }
   ],
   "source": [
    "val test = sc.textFile(\"test.txt\").option(\"header\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\n",
      "0\t2\n",
      "0\t3\n",
      "0\t4\n",
      "0\t5\n",
      "0\t6\n",
      "0\t7\n",
      "0\t8\n",
      "0\t9\n",
      "0\t10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "header: String = # FromNodeId\tToNodeId\n",
       "data: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[10] at filter at <console>:30\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var header = test.first()\n",
    "var data = test.filter(row => row != header)\n",
    "data.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quiz1: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[16] at map at <console>:26\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var quiz1 = data.map(_.split('\\t')).map(pair => (pair(0),pair(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,1)\n",
      "(0,2)\n",
      "(0,3)\n",
      "(0,4)\n",
      "(0,5)\n",
      "(0,6)\n",
      "(0,7)\n",
      "(0,8)\n",
      "(0,9)\n",
      "(0,10)\n"
     ]
    }
   ],
   "source": [
    "quiz1.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "2: error: identifier expected but 'type' found.",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: identifier expected but 'type' found.",
      "quiz1.take(1).type()",
      "              ^",
      ""
     ]
    }
   ],
   "source": [
    "quiz1.take(1).type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(url_4,CompactBuffer(url_3, url_1))\n",
      "(url_2,CompactBuffer(url_1))\n",
      "(url_3,CompactBuffer(url_2, url_1))\n",
      "(url_1,CompactBuffer(url_4))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[146] at rdd at <console>:31\n",
       "header: String = a\tb\n",
       "lines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[147] at filter at <console>:33\n",
       "pairs: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[148] at map at <console>:34\n",
       "links: org.apache.spark.rdd.RDD[(String, Iterable[String])] = ShuffledRDD[152] at groupByKey at <console>:38\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = spark.read.textFile(\"a.txt\").rdd\n",
    "var header = file.first()\n",
    "var lines = file.filter(row => row != header)\n",
    "val pairs = lines.map{ s =>\n",
    "      val parts = s.split(\"\\\\s+\")               // Splits a line into an array of 2 elements according space(s)\n",
    "             (parts(0), parts(1))                 // create the parts<url, url> for each line in the file\n",
    "    }\n",
    "val links = pairs.distinct().groupByKey()\n",
    "links.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ranks: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[153] at mapValues at <console>:26\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var ranks = links.mapValues(v => 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(url_4,1.0)\n",
      "(url_2,1.0)\n",
      "(url_3,1.0)\n",
      "(url_1,1.0)\n"
     ]
    }
   ],
   "source": [
    "ranks.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i <- 1 to iters) {\n",
    "    val contribs = links.join(ranks)         // join  -> RDD1\n",
    "         .values                           // extract values from RDD1 -> RDD2          \n",
    "         .flatMap{ case (urls, rank) =>    // RDD2 -> conbrib RDD\n",
    "                 val size = urls.size        \n",
    "                     urls.map(url => (url, rank / size))   // the ranks are distributed equally amongs the various URLs\n",
    "             }\n",
    "    ranks = contribs.reduceByKey(_ + _).mapValues(0.15 + 0.85 * _) // ranks RDD\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,CompactBuffer(2, 9, 15, 19, 17, 13, 3, 7, 11, 5, 10, 1, 8, 12, 6, 16, 14, 18, 4))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tfile: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[136] at rdd at <console>:31\n",
       "Theader: String = # FromNodeId\tToNodeId\n",
       "Tlines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[137] at filter at <console>:33\n",
       "Tpairs: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[138] at map at <console>:34\n",
       "Tlinks: org.apache.spark.rdd.RDD[(String, Iterable[String])] = ShuffledRDD[142] at groupByKey at <console>:38\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Tfile = spark.read.textFile(\"test.txt\").rdd\n",
    "var Theader = Tfile.first()\n",
    "var Tlines = Tfile.filter(row => row != Theader)\n",
    "val Tpairs = Tlines.map{ s =>\n",
    "      val parts = s.split(\"\\t\")               // Splits a line into an array of 2 elements according space(s)\n",
    "             (parts(0), parts(1))                 // create the parts<url, url> for each line in the file\n",
    "    }\n",
    "val Tlinks = Tpairs.distinct().groupByKey().cache()\n",
    "Tlinks.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tranks: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[155] at mapValues at <console>:26\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var Tranks = Tlinks.mapValues(v => 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,1.0)\n"
     ]
    }
   ],
   "source": [
    "Tranks.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
