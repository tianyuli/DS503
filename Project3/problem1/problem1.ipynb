{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.types._\n",
       "transactionsRDD: org.apache.spark.rdd.RDD[String] = test2.csv MapPartitionsRDD[128] at textFile at <console>:43\n",
       "schemaString: String = TransID CustID TransTotal TransNumItems TransDesc\n",
       "fields: Array[org.apache.spark.sql.types.StructField] = Array(StructField(TransID,StringType,true), StructField(CustID,StringType,true), StructField(TransTotal,StringType,true), StructField(TransNumItems,StringType,true), StructField(TransDesc,StringType,true))\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(TransID,StringType,true), StructField(CustID,StringType,true), StructField(TransTotal,StringType,true), StructField(TransNumItems,StringType,true), StructField(TransDesc,StringType,true))\n",
       "rowRDD: org.apache.spar..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val transactionsRDD = sc.textFile(\"test2.csv\")\n",
    "val schemaString = \"TransID CustID TransTotal TransNumItems TransDesc\"\n",
    "\n",
    "val fields = schemaString.split(\" \").\n",
    "    map(fieldName => StructField(fieldName, StringType, nullable = true))\n",
    "val schema = StructType(fields)\n",
    "\n",
    "val rowRDD = transactionsRDD.\n",
    "    map(_.split(\",\")).\n",
    "    map(attributes => Row(attributes(0), attributes(1), attributes(2), attributes(3), attributes(4).trim))\n",
    "val transactionsDF = spark.createDataFrame(rowRDD, schema)\n",
    "transactionsDF.createOrReplaceTempView(\"transactions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1:\tFilter\tout\t(drop)\tthe\ttransactions\tfrom\tT whose\ttotal\tamount\tis\tless\tthan\t$200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T1: org.apache.spark.sql.DataFrame = [TransID: string, CustID: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val T1 = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM transactions\n",
    "    WHERE TransTotal > 200\n",
    "    \"\"\")\n",
    "transactionsDF.createOrReplaceTempView(\"T1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+-------------+--------------------+\n",
      "|TransID|CustID|TransTotal|TransNumItems|           TransDesc|\n",
      "+-------+------+----------+-------------+--------------------+\n",
      "|      1| 13931| 947.27325|           10|7tk17k7r14ixnkgbw9xc|\n",
      "|      3| 10311|  563.8975|           10|9yc74al32gyx4hmpu...|\n",
      "|      4| 14630|  737.5072|            4|bdnglgyulcln7zl9i...|\n",
      "|      5| 38528| 520.28046|            3|9nvr6vb2ouppqqv8l...|\n",
      "|      6|  4505|  694.3501|            6|j6eoz2r5swlpjpxkq...|\n",
      "|      7| 12466|  283.8117|            8|wrwfqu49uakdxqq2c...|\n",
      "|      8| 16572|  331.2615|            6|7bq937yzswhhlejos...|\n",
      "|      9| 18696|  380.0146|            1|v77phaie3detj3tm5...|\n",
      "|     10| 11084|  781.5895|            1|f5sj0wmi9pmmmkc5w...|\n",
      "|     11|  8115|  958.5794|            2|igxkh8nzn7iralfn4...|\n",
      "|     13| 36239|  302.1748|            9|iyo7s15aib3ll9ug3...|\n",
      "|     14| 45097| 206.60675|            5|dlcq63uctjfrtyjvc...|\n",
      "|     15| 23428|  907.9976|            7|hura8k4tv7w8u6hbj...|\n",
      "|     16| 15586|  846.5182|            2|2qx42jju6gn2i3tw6...|\n",
      "|     17| 18846| 329.52203|            8|15hrrpvzsz7isjvip...|\n",
      "|     18| 32477|  235.0295|            5|560j5i030xmykpyao...|\n",
      "|     19| 14422| 643.40985|            4|cgrp95ubdh6l0z978...|\n",
      "|     20| 15024|  948.1538|            3|nhagydrshd892xw6i...|\n",
      "|     21| 17630| 364.04175|            6|u3nk33tpexv8msyoo...|\n",
      "|     22| 46609|  718.1135|            1|o8k8bw8tf3luiuwd7...|\n",
      "+-------+------+----------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2:\tOver\tT1,\tgroup\tthe\ttransactions\tby\tthe\tNumber\tof\tItems\tit\thas,\tand\tfor\teach\tgroup\tcalculate\tthe\tsum\tof\ttotal\tamounts,\tthe\taverage\tof\ttotal\tamounts,\tthe\tmin\tand\tthe\tmax\tof\tthe\ttotal\tamounts.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T2: org.apache.spark.sql.DataFrame = [TransNumItems: string, sum: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var T2 = spark.sql(\"\"\"\n",
    "    SELECT TransNumItems, sum(TransTotal) as sum, avg(TransTotal) as avg, min(TransTotal) as min, max(TransTotal) as max\n",
    "    FROM T1\n",
    "    GROUP BY TransNumItems\n",
    "    \"\"\")\n",
    "transactionsDF.createOrReplaceTempView(\"T2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Report\tback\tT2\tto\tthe\tclient\tside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+------------------+---------+---------+\n",
      "|TransNumItems|               sum|               avg|      min|      max|\n",
      "+-------------+------------------+------------------+---------+---------+\n",
      "|            7| 503184.9979949998| 507.7547911150351|10.006294|999.63135|\n",
      "|            3| 513023.1376229998|510.97922074003964|10.366854| 999.9228|\n",
      "|            8| 496979.5531750001|497.97550418336687|100.24613| 999.2891|\n",
      "|            5| 500082.7098709998| 496.1137994751982| 10.64183| 998.9041|\n",
      "|            6| 515128.0622620001|508.01584049506914|10.054651| 999.7996|\n",
      "|            9| 529327.5989350002|503.16311685836524|10.484435| 996.2176|\n",
      "|            1|516024.74242849986| 500.0239752214146|100.03025| 997.6773|\n",
      "|           10| 450016.9132989999| 498.9101034356983|100.17128| 998.7193|\n",
      "|            4| 503476.3507269997| 506.5154433873236|100.71896|998.41284|\n",
      "|            2| 512846.8069620001|510.29533031044787| 100.8176|999.82263|\n",
      "+-------------+------------------+------------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) T3:\tOver\tT1,\tgroup\tthe\ttransactions\tby\tcustomer\tID,\tand\tfor\teach\tgroup\treport\tthe\tcustomer\tID,\tand\tthe\ttransactions’\tcount.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|CustID|TransCount|\n",
      "+------+----------+\n",
      "| 18726|         1|\n",
      "| 20569|         1|\n",
      "| 48402|         1|\n",
      "| 19095|         1|\n",
      "| 32275|         1|\n",
      "|  7273|         1|\n",
      "| 18556|         1|\n",
      "|  3414|         2|\n",
      "| 31432|         2|\n",
      "| 31713|         2|\n",
      "|  3210|         1|\n",
      "|  4821|         1|\n",
      "| 44446|         1|\n",
      "| 29549|         2|\n",
      "| 21452|         1|\n",
      "| 14899|         1|\n",
      "| 45157|         2|\n",
      "| 22121|         1|\n",
      "| 21889|         1|\n",
      "| 39659|         1|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T3: org.apache.spark.sql.DataFrame = [CustID: string, TransCount: bigint]\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var T3 = spark.sql(\"\"\"\n",
    "    SELECT CustID, count(*) as TransCount\n",
    "    FROM T1\n",
    "    GROUP BY CustID\n",
    "    \"\"\")\n",
    "transactionsDF.createOrReplaceTempView(\"T3\")\n",
    "T3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) T4:\tFilter\tout\t(drop)\tthe\ttransactions\tfrom\tT whose\ttotal\tamount\tis\tless\tthan $600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T4: org.apache.spark.sql.DataFrame = [TransID: string, CustID: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var T4 = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM transactions\n",
    "    WHERE TransTotal < 600\n",
    "    \"\"\")\n",
    "transactionsDF.createOrReplaceTempView(\"T4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) T5:\tOver\tT4,\tgroup\tthe\ttransactions\tby\tcustomer\tID,\tand\tfor\teach\tgroup\treport\tthe\tcustomer\tID,\tand\tthe\ttransactions’\tcount.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|CustID|TransCount|\n",
      "+------+----------+\n",
      "| 18726|         1|\n",
      "| 20569|         1|\n",
      "| 48402|         1|\n",
      "| 19095|         1|\n",
      "| 32275|         1|\n",
      "|  7273|         1|\n",
      "| 18556|         1|\n",
      "|  3414|         2|\n",
      "| 31432|         2|\n",
      "| 31713|         2|\n",
      "|  3210|         1|\n",
      "|  4821|         1|\n",
      "| 44446|         1|\n",
      "| 29549|         2|\n",
      "| 21452|         1|\n",
      "| 14899|         1|\n",
      "| 45157|         2|\n",
      "| 22121|         1|\n",
      "| 21889|         1|\n",
      "| 39659|         1|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5: org.apache.spark.sql.DataFrame = [CustID: string, TransCount: bigint]\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var T5 = spark.sql(\"\"\"\n",
    "    SELECT CustID, count(*) as TransCount\n",
    "    FROM T4\n",
    "    GROUP BY CustID\n",
    "    \"\"\")\n",
    "transactionsDF.createOrReplaceTempView(\"T5\")\n",
    "T5.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) T6:\tSelect\tthe\tcustomer\tIDs\twhose\t\tT5.count\t*\t5 <\tT3.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " cannot resolve '`T5.TransCount`' given input columns: [t5.TransID, t5.TransDesc, t3.TransDesc, t3.TransTotal, t5.TransTotal, t3.TransID, t5.TransNumItems, t3.TransNumItems, t5.CustID, t3.CustID]; line 6 pos 10;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: cannot resolve '`T5.TransCount`' given input columns: [t5.TransID, t5.TransDesc, t3.TransDesc, t3.TransTotal, t5.TransTotal, t3.TransID, t5.TransNumItems, t3.TransNumItems, t5.CustID, t3.CustID]; line 6 pos 10;",
      "'Aggregate ['T5.CustID], ['T5.CustID]",
      "+- 'Filter (('T5.TransCount * 5) < 'T3.TransCount)",
      "   +- Join Inner, (CustID#419 = CustID#512)",
      "      :- SubqueryAlias `t5`",
      "      :  +- LogicalRDD [TransID#418, CustID#419, TransTotal#420, TransNumItems#421, TransDesc#422], false",
      "      +- SubqueryAlias `t3`",
      "         +- LogicalRDD [TransID#511, CustID#512, TransTotal#513, TransNumItems#514, TransDesc#515], false",
      "",
      "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)",
      "  at scala.collection.immutable.List.foreach(List.scala:392)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)",
      "  ... 42 elided",
      ""
     ]
    }
   ],
   "source": [
    "var T6 = spark.sql(\"\"\"\n",
    "    SELECT T5.CustID\n",
    "    FROM T5\n",
    "    JOIN T3\n",
    "    ON T5.CustID = T3.CustID\n",
    "    WHERE T5.TransCount * 5 < T3.TransCount\n",
    "    GROUP BY T5.CustID\n",
    "    \"\"\")\n",
    "transactionsDF.createOrReplaceTempView(\"T6\")\n",
    "T6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
